# -*- coding: utf-8 -*-
"""RL HW3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f7WkZIIjwvrEjZ6B5JPq9A9mDLv9OeJQ

# install library
"""

# install
!pip install git+https://github.com/aravindr93/mjrl@master#egg=mjrl
!pip install gym<0.24.0
!pip install mujoco
!pip install -f https://download.pytorch.org/whl/torch_stable.html \
                free-mujoco-py \
                einops \
                protobuf==3.20.1 \
                git+https://github.com/rail-berkeley/d4rl.git \
                mediapy \
                Pillow==9.0.0
!pip install gym torch tensorboard

"""# ddpg.py

## import
"""

# Spring 2024, 535514 Reinforcement Learning
# HW3: DDPG

import sys
import gym
import numpy as np
import os
import time
import random
from collections import namedtuple
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.autograd import Variable
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter

"""## install"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -q swig
# !pip install -q gymnasium[box2d]
!pip install box2d-py
!git clone https://github.com/pybox2d/pybox2d
# %cd pybox2d
!python setup.py build
!python setup.py install

"""## store"""

from google.colab import drive
drive.mount('/content/drive')
import os
save_path = '/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg/'
if not os.path.exists(save_path):
    os.makedirs(save_path)

"""## ddpg"""

# Define a tensorboard writer
writer = SummaryWriter(save_path)

def soft_update(target, source, tau):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

def hard_update(target, source):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(param.data)

Transition = namedtuple(
    'Transition', ('state', 'action', 'mask', 'next_state', 'reward'))

class ReplayMemory(object):

    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
        self.position = 0

    def push(self, *args):
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[self.position] = Transition(*args)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class OUNoise:

    def __init__(self, action_dimension, scale=0.1, mu=0, theta=0.15, sigma=0.2):
        self.action_dimension = action_dimension
        self.scale = scale
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.state = np.ones(self.action_dimension) * self.mu
        self.reset()

    def reset(self):
        self.state = np.ones(self.action_dimension) * self.mu

    def noise(self):
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))
        self.state = x + dx
        return self.state * self.scale

class Actor(nn.Module):
    def __init__(self, hidden_size, num_inputs, action_space):
        super(Actor, self).__init__()
        self.action_space = action_space
        num_outputs = action_space.shape[0]

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Construct your own actor network
        self.fc1 = nn.Linear(num_inputs, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_outputs)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()


        ########## END OF YOUR CODE ##########

    def forward(self, inputs):

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Define the forward pass your actor network
        x = self.relu(self.fc1(inputs))
        x = self.relu(self.fc2(x))
        x = self.tanh(self.fc3(x))
        return x

        ########## END OF YOUR CODE ##########

class Critic(nn.Module):
    def __init__(self, hidden_size, num_inputs, action_space):
        super(Critic, self).__init__()
        self.action_space = action_space
        num_outputs = action_space.shape[0]

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Construct your own critic network
        self.fc1 = nn.Linear(num_inputs + num_outputs, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)
        self.relu = nn.ReLU()


        ########## END OF YOUR CODE ##########

    def forward(self, inputs, actions):

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Define the forward pass your critic network
        x = torch.cat([inputs, actions], 1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

        ########## END OF YOUR CODE ##########


class DDPG(object):
    def __init__(self, num_inputs, action_space, gamma=0.995, tau=0.0005, hidden_size=128, lr_a=1e-4, lr_c=1e-3):

        self.num_inputs = num_inputs
        self.action_space = action_space

        self.actor = Actor(hidden_size, self.num_inputs, self.action_space)
        self.actor_target = Actor(hidden_size, self.num_inputs, self.action_space)
        self.actor_perturbed = Actor(hidden_size, self.num_inputs, self.action_space)
        self.actor_optim = Adam(self.actor.parameters(), lr=lr_a)

        self.critic = Critic(hidden_size, self.num_inputs, self.action_space)
        self.critic_target = Critic(hidden_size, self.num_inputs, self.action_space)
        self.critic_optim = Adam(self.critic.parameters(), lr=lr_c)

        self.gamma = gamma
        self.tau = tau

        hard_update(self.actor_target, self.actor)
        hard_update(self.critic_target, self.critic)


    def select_action(self, state, action_noise=None):
        self.actor.eval()
        mu = self.actor((Variable(state)))
        mu = mu.data

        ########## YOUR CODE HERE (3~5 lines) ##########
        # Add noise to your action for exploration
        # Clipping might be needed
        self.actor.eval()

        if action_noise is not None:
            mu = self.actor((state))
            mu = mu.data
            mu += torch.Tensor(action_noise.noise())

        return mu.clamp(-1, 1)


        ########## END OF YOUR CODE ##########


    def update_parameters(self, batch):
        state_batch = torch.cat([transition.state for transition in batch])
        action_batch = torch.cat([transition.action for transition in batch])
        reward_batch = torch.cat([transition.reward for transition in batch])
        mask_batch = torch.cat([transition.mask for transition in batch])
        next_state_batch = torch.cat([transition.next_state for transition in batch])


        ########## YOUR CODE HERE (10~20 lines) ##########
        # Calculate policy loss and value loss
        # Update the actor and the critic

        next_action_batch = self.actor_target(next_state_batch)
        next_state_action_values = self.critic_target(next_state_batch, next_action_batch)
        expected_state_action_values = reward_batch.unsqueeze(1) + (self.gamma * next_state_action_values * mask_batch.unsqueeze(1))
        state_action_values = self.critic(state_batch, action_batch)

        value_loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.detach())

        self.critic_optim.zero_grad()
        value_loss.backward()
        self.critic_optim.step()

        policy_loss = -self.critic(state_batch, self.actor(state_batch)).mean()

        self.actor_optim.zero_grad()
        policy_loss.backward()
        self.actor_optim.step()


        ########## END OF YOUR CODE ##########

        soft_update(self.actor_target, self.actor, self.tau)
        soft_update(self.critic_target, self.critic, self.tau)

        return value_loss.item(), policy_loss.item()


    def save_model(self, env_name, suffix="", actor_path=None, critic_path=None):
        local_time = time.localtime()
        timestamp = time.strftime("%m%d%Y_%H%M%S", local_time)
        if not os.path.exists('/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg/'):
            os.makedirs('/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg/')

        if actor_path is None:
            actor_path = "/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_actor_{}_{}_{}".format(env_name, timestamp, suffix)
        if critic_path is None:
            critic_path = "/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_critic_{}_{}_{}".format(env_name, timestamp, suffix)
        print('Saving models to {} and {}'.format(actor_path, critic_path))
        torch.save(self.actor.state_dict(), actor_path)
        torch.save(self.critic.state_dict(), critic_path)

    def load_model(self, actor_path, critic_path):
        print('Loading models from {} and {}'.format(actor_path, critic_path))
        if actor_path is not None:
            self.actor.load_state_dict(torch.load(actor_path))
        if critic_path is not None:
            self.critic.load_state_dict(torch.load(critic_path))

def train():
    num_episodes = 200
    gamma = 0.995
    tau = 0.002
    hidden_size = 128
    noise_scale = 0.3
    replay_size = 100000
    batch_size = 128
    updates_per_step = 1
    print_freq = 20
    ewma_reward = 0
    rewards = []
    ewma_reward_history = []
    total_numsteps = 0
    updates = 0


    agent = DDPG(env.observation_space.shape[0], env.action_space, gamma, tau, hidden_size)
    ounoise = OUNoise(env.action_space.shape[0])
    memory = ReplayMemory(replay_size)

    for i_episode in range(num_episodes):

        ounoise.scale = noise_scale
        ounoise.reset()

        state = torch.Tensor([env.reset()])

        episode_reward = 0
        while True:

            ########## YOUR CODE HERE (15~25 lines) ##########
            # 1. Interact with the env to get new (s,a,r,s') samples
            action = agent.select_action(state, ounoise)
            next_state, reward, done, _ = env.step(action.numpy()[0])
            next_state = torch.Tensor([next_state])
            mask = torch.Tensor([not done])
            reward = torch.Tensor([reward])


            # 2. Push the sample to the replay buffer
            memory.push(state, action, mask, next_state, reward)

            state = next_state
            episode_reward += reward[0]
            total_numsteps += 1


            # 3. Update the actor and the critic
            if len(memory) > batch_size:
                for _ in range(updates_per_step):
                    batch = memory.sample(batch_size)
                    critic_loss, actor_loss = agent.update_parameters(batch)
                    updates += 1

            if done:
                break


            ########## END OF YOUR CODE ##########


        rewards.append(episode_reward)
        t = 0
        if i_episode % print_freq == 0:
            state = torch.Tensor([env.reset()])
            episode_reward = 0
            while True:
                action = agent.select_action(state)

                next_state, reward, done, _ = env.step(action.numpy()[0])

                env.render()

                episode_reward += reward

                next_state = torch.Tensor([next_state])

                state = next_state

                t += 1
                if done:
                    break

            rewards.append(episode_reward)
            # update EWMA reward and log the results
            ewma_reward = 0.05 * episode_reward + (1 - 0.05) * ewma_reward
            ewma_reward_history.append(ewma_reward)
            print("Episode: {}, length: {}, reward: {:.2f}, ewma reward: {:.2f}".format(i_episode, t, rewards[-1], ewma_reward))

        writer.add_scalar('reward', rewards[-1], i_episode)
        writer.add_scalar('ep_reward', ewma_reward, i_episode)
        writer.add_scalar('episode_length', t, i_episode)


    agent.save_model('Pendulum-v1', '.pth')


if __name__ == '__main__':
    # For reproducibility, fix the random seed
    random_seed = 10
    env = gym.make('Pendulum-v1')
    env.seed(random_seed)
    torch.manual_seed(random_seed)
    train()

"""## visualize"""

# Commented out IPython magic to ensure Python compatibility.
# !pip install tensorboard

# from tensorboard import program
# import tensorflow as tf

# %load_ext tensorboard
# %reload_ext tensorboard
# %tensorboard --logdir '/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg/'

import os
os.listdir('/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg/')

"""# ddpg_cheetah.py

## import
"""

# Spring 2024, 535514 Reinforcement Learning
# HW3: DDPG
!apt-get install patchelf

!pip install glfw
!apt-get install -y \
    libgl1-mesa-dev \
    libgl1-mesa-glx \
    libglew-dev \
    libosmesa6-dev \
    software-properties-common

import torch
import sys
import gym
import numpy as np
import os
import time
import random
import mujoco_py
from collections import namedtuple
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.autograd import Variable
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from collections import deque
import matplotlib.pyplot as plt
from IPython import display

"""## store"""

from google.colab import drive
drive.mount('/content/drive')
import os
save_path = '/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_cheetah/'
if not os.path.exists(save_path):
    os.makedirs(save_path)

"""## ddpg_cheetah"""

# Define a tensorboard writer
writer = SummaryWriter(save_path)

def soft_update(target, source, tau):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

def hard_update(target, source):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(param.data)

Transition = namedtuple(
    'Transition', ('state', 'action', 'mask', 'next_state', 'reward'))

class ReplayMemory(object):

    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
        self.position = 0

    def push(self, *args):
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[self.position] = Transition(*args)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class OUNoise:

    def __init__(self, action_dimension, scale=0.1, mu=0, theta=0.15, sigma=0.2):
        self.action_dimension = action_dimension
        self.scale = scale
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.state = np.ones(self.action_dimension) * self.mu
        self.reset()

    def reset(self):
        self.state = np.ones(self.action_dimension) * self.mu

    def noise(self):
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))
        self.state = x + dx
        return self.state * self.scale

class Actor(nn.Module):
    def __init__(self, hidden_size, num_inputs, action_space):
        super(Actor, self).__init__()
        self.action_space = action_space
        num_outputs = action_space.shape[0]

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Construct your own actor network
        self.fc1 = nn.Linear(num_inputs, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_outputs)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()


        ########## END OF YOUR CODE ##########

    def forward(self, inputs):

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Define the forward pass your actor network
        x = self.relu(self.fc1(inputs))
        x = self.relu(self.fc2(x))
        x = self.tanh(self.fc3(x))
        return x

        ########## END OF YOUR CODE ##########

class Critic(nn.Module):
    def __init__(self, hidden_size, num_inputs, action_space):
        super(Critic, self).__init__()
        self.action_space = action_space
        num_outputs = action_space.shape[0]

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Construct your own critic network
        self.fc1 = nn.Linear(num_inputs + num_outputs, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)
        self.relu = nn.ReLU()


        ########## END OF YOUR CODE ##########

    def forward(self, inputs, actions):

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Define the forward pass your critic network
        x = torch.cat([inputs, actions], 1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

        ########## END OF YOUR CODE ##########


class DDPG(object):
    def __init__(self, num_inputs, action_space, gamma=0.995, tau=0.0005, hidden_size=128, lr_a=1e-4, lr_c=1e-3):

        self.num_inputs = num_inputs
        self.action_space = action_space

        self.actor = Actor(hidden_size, self.num_inputs, self.action_space)
        self.actor_target = Actor(hidden_size, self.num_inputs, self.action_space)
        self.actor_perturbed = Actor(hidden_size, self.num_inputs, self.action_space)
        self.actor_optim = Adam(self.actor.parameters(), lr=lr_a)

        self.critic = Critic(hidden_size, self.num_inputs, self.action_space)
        self.critic_target = Critic(hidden_size, self.num_inputs, self.action_space)
        self.critic_optim = Adam(self.critic.parameters(), lr=lr_c)

        self.gamma = gamma
        self.tau = tau

        hard_update(self.actor_target, self.actor)
        hard_update(self.critic_target, self.critic)


    def select_action(self, state, action_noise=None):
        self.actor.eval()
        mu = self.actor((Variable(state)))
        mu = mu.data

        ########## YOUR CODE HERE (3~5 lines) ##########
        # Add noise to your action for exploration
        # Clipping might be needed
        self.actor.eval()

        if action_noise is not None:
            mu = self.actor((state))
            mu = mu.data
            mu += torch.Tensor(action_noise.noise())

        return mu.clamp(-1, 1)


        ########## END OF YOUR CODE ##########


    def update_parameters(self, batch):
        state_batch = torch.cat([transition.state for transition in batch])
        action_batch = torch.cat([transition.action for transition in batch])
        reward_batch = torch.cat([transition.reward for transition in batch])
        mask_batch = torch.cat([transition.mask for transition in batch])
        next_state_batch = torch.cat([transition.next_state for transition in batch])


        ########## YOUR CODE HERE (10~20 lines) ##########
        # Calculate policy loss and value loss
        # Update the actor and the critic

        next_action_batch = self.actor_target(next_state_batch)
        next_state_action_values = self.critic_target(next_state_batch, next_action_batch)
        expected_state_action_values = reward_batch.unsqueeze(1) + (self.gamma * next_state_action_values * mask_batch.unsqueeze(1))
        state_action_values = self.critic(state_batch, action_batch)

        value_loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.detach())

        self.critic_optim.zero_grad()
        value_loss.backward()
        self.critic_optim.step()

        policy_loss = -self.critic(state_batch, self.actor(state_batch)).mean()

        self.actor_optim.zero_grad()
        policy_loss.backward()
        self.actor_optim.step()


        ########## END OF YOUR CODE ##########

        soft_update(self.actor_target, self.actor, self.tau)
        soft_update(self.critic_target, self.critic, self.tau)

        return value_loss.item(), policy_loss.item()


    def save_model(self, env_name, suffix="", actor_path=None, critic_path=None):
        local_time = time.localtime()
        timestamp = time.strftime("%m%d%Y_%H%M%S", local_time)
        if not os.path.exists('/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_cheetah/'):
            os.makedirs('/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_cheetah/')

        if actor_path is None:
            actor_path = "/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_cheetah_actor_{}_{}_{}".format(env_name, timestamp, suffix)
        if critic_path is None:
            critic_path = "/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_cheetah_critic_{}_{}_{}".format(env_name, timestamp, suffix)
        print('Saving models to {} and {}'.format(actor_path, critic_path))
        torch.save(self.actor.state_dict(), actor_path)
        torch.save(self.critic.state_dict(), critic_path)

    def load_model(self, actor_path, critic_path):
        print('Loading models from {} and {}'.format(actor_path, critic_path))
        if actor_path is not None:
            self.actor.load_state_dict(torch.load(actor_path))
        if critic_path is not None:
            self.critic.load_state_dict(torch.load(critic_path))

def show_state(env, state, step, episode_reward, ewma_reward):
    plt.figure(figsize=(8, 6))
    plt.imshow(env.render(mode='rgb_array'))
    plt.title("Step: {}, Episode Reward: {:.2f}, EWMA Reward: {:.2f}".format(step, episode_reward, ewma_reward))
    plt.axis('off')
    plt.show()

def train():
    num_episodes = 200
    gamma = 0.995
    tau = 0.002
    hidden_size = 128
    noise_scale = 0.3
    replay_size = 100000
    batch_size = 128
    updates_per_step = 1
    print_freq = 1
    ewma_reward = 0
    rewards = []
    ewma_reward_history = []
    total_numsteps = 0
    updates = 0

    env = gym.make('HalfCheetah-v2')
    agent = DDPG(env.observation_space.shape[0], env.action_space, gamma, tau, hidden_size)
    ounoise = OUNoise(env.action_space.shape[0])
    memory = ReplayMemory(replay_size)

    for i_episode in range(num_episodes):

        ounoise.scale = noise_scale
        ounoise.reset()

        state = torch.Tensor([env.reset()])

        episode_reward = 0
        while True:

            ########## YOUR CODE HERE (15~25 lines) ##########
            # 1. Interact with the env to get new (s,a,r,s') samples
            action = agent.select_action(state, ounoise)
            next_state, reward, done, _ = env.step(action.numpy()[0])
            next_state = torch.Tensor([next_state])
            mask = torch.Tensor([not done])
            reward = torch.Tensor([reward])


            # 2. Push the sample to the replay buffer
            memory.push(state, action, mask, next_state, reward)

            state = next_state
            episode_reward += reward[0]
            total_numsteps += 1


            # 3. Update the actor and the critic
            if len(memory) > batch_size:
                for _ in range(updates_per_step):
                    batch = memory.sample(batch_size)
                    critic_loss, actor_loss = agent.update_parameters(batch)
                    updates += 1

            if done:
                break


            ########## END OF YOUR CODE ##########


        rewards.append(episode_reward)
        t = 0
        if i_episode % print_freq == 0:
            state = torch.Tensor([env.reset()])
            episode_reward = 0
            while True:
                action = agent.select_action(state)

                next_state, reward, done, _ = env.step(action.numpy()[0])

                # show_state(env, next_state, t, episode_reward, ewma_reward)

                episode_reward += reward

                next_state = torch.Tensor([next_state])

                state = next_state

                t += 1
                if done:
                    break

            rewards.append(episode_reward)
            # update EWMA reward and log the results
            ewma_reward = 0.05 * episode_reward + (1 - 0.05) * ewma_reward
            ewma_reward_history.append(ewma_reward)
            print("Episode: {}, length: {}, reward: {:.2f}, ewma reward: {:.2f}".format(i_episode, t, rewards[-1], ewma_reward))

        writer.add_scalar('reward', rewards[-1], i_episode)
        writer.add_scalar('ep_reward', ewma_reward, i_episode)
        writer.add_scalar('episode_length', t, i_episode)


    agent.save_model('HalfCheetah-v2', '.pth')


if __name__ == '__main__':
    # For reproducibility, fix the random seed
    random_seed = 10
    env = gym.make("HalfCheetah-v3")
    env.seed(random_seed)
    torch.manual_seed(random_seed)
    train()

"""## visualize"""

# Commented out IPython magic to ensure Python compatibility.
# !pip install tensorboard

# from tensorboard import program
# import tensorflow as tf

# %load_ext tensorboard
# %tensorboard --logdir '/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_cheetah/'
!kill 48582
# %reload_ext tensorboard
# %tensorboard --logdir '/content/drive/My Drive/資訊工程學習資料/強化學習原理/課程作業 (謝秉均)/HW3/ddpg_cheetah/'